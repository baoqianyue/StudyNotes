## 神经网络   

### 反向传播(Backpropation)      

* 三种基本计算结点   

  * 加法运算

    在反向传播过程中，不管输入值是多少，该结点只会将传回的梯度值均匀的分配给之前与它相连的所有路径，因为加法的偏导都是+1.0        

  * max运算 

    max结点在反向传播过程中，只会将传回的梯度值分配给与它相连的其中一条路径，这条路径的输入值是所有输入值的最大值，其他路径上的梯度都为0       

  * 乘法运算 

    乘法结点在反向传播过程中，如果只连接了两个输入路径，那相当于`x*y` ,那各自路径上的梯度值就是传回的梯度值乘上相应的偏导，`x*y`对`x`的偏导是`y`，`x*y`对`y`的偏导是`x`         

* 前向计算和反向传播API封装  
   * Graph(or Net) object    

     ```python
     class ComputationalGraph(objcet):
     	def forward(inputs):
     		# 1. [pass inputs to input gates...]
     		# 2. forward the computational graph:
     		for gate in self.graph.nodes_topologically_sorted():
     			gate.forward()
     		return loss # the final gate in the graph outputs the loss   
     	def backward():
     		for gate in reversed(self.graph.nodes_topologically_sorted()):
     			gate.backward() # little piece of backprop(chain rule applied)
     		return inputs_gradients   
     ```

     

   * MultiplyGate(乘法结点)    

     ![1](./image/backpro1.png)

     ```python
     class MutiplyGate(object):
         def forward(x, y):
             z = x * y
             self.x = x # must keep these around 保存起来为了反向时计算梯度
             self.y = y
             return z 
        	def backward(dz):
             dx = self.y * dz # [dz/dx * dL/dz] 这里dL/dz=dz/dz=1
             dy = self.x * dz # [dz/dy * dL.dz]
             return [dx, dy]
     ```

### 神经网络(Neural Networks)      

* 神经网络和之前的逻辑回归、SVM的区别   

  * (Before) Linear score function:  

    
    $$
    f = Wx
    $$

  * (Now) 2-layer Neural Network: 

    
    $$
    f = W_2 max(0,W_1x)
    $$
    ![net](./image/net1.png)

    * 1. 我们通过学习训练集得到权重`W1`,对应于一些类别的模板(直接将`W1`中的一列数据进行绘制)，上图下部只是每个类别中的一个模板(template)      
    * 2. 输入新的数据`X`,结合`W1`，计算得到一个得分score(可能再经过一些非线性变换，如relu，sigmod等激活函数)，也就是`h`      
    * 3. 得到h后，由于一个类别对应很多个模板(template),我们用`W2`作为权重在对这些**同一类别的不同模板**对应的score进行运算得到最终的对应于这个输入X的score         

