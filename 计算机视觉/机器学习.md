## 监督学习    

## 非监督学习    

## 半监督学习     

## 增强学习    

## 批量学习(BatchLearning)

## 在线学习(OnlineLearing)    

## 参数学习(Parametric Learning)  

## 非参数学习(Nonparametric Learning)   

## 特征工程    

## 奥卡姆剃刀
    简单的就是好的    

## 没有免费的午餐定理    
    可以严格的数学推导出：任意两个算法，他们的期望性能是相同的      

## Label Smoothing Regularization     

[解释](https://zhuanlan.zhihu.com/p/27678647)   

## Dropout解决过拟合问题     

[解释](https://zhuanlan.zhihu.com/p/23178423)     

在网络的隐藏层中随机删除一半的神经元，然后前向传播，反向传播，更新已有神经元的参数，然后恢复上一步删除掉的神经元，重复这一过程。      

其中有一个取平均的过程，删除一部分神经元后，相当于在训练一个不同的网络，因为不同的网络有可能产生不同的过拟合，取平均有可能让一些相反的过拟合相互抵消。

    